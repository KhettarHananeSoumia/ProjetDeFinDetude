from cProfile import label
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pytest import cmdline
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn import metrics
from sklearn import svm
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.decomposition import PCA
import nltk, re, unicodedata
from nltk.corpus import stopwords
from nltk import word_tokenize
import string
from nltk.stem import PorterStemmer
from mlxtend.plotting import plot_decision_regions
import eli5 as eli
from sklearn.multiclass import OneVsOneClassifier
import string , spacy
from spacy.lang.fr.stop_words import STOP_WORDS
from spacy.lang.fr import French
from sklearn.base import TransformerMixin
from nltk.stem import LancasterStemmer, WordNetLemmatizer
########################
 
####################

#path to the dataset
path1 = 'D:\MyDataSet.txt'

#read the data and give the name to each column
data1 = pd.read_csv(path1, header=None,names=[
    'Submitting Device',
    'Device User',
    'Form Completed',
    'Upload Completed',
    'Submission ID',
    'Submission ID generated by device',
    'A.Fiche signalétique : A-1.Numéro parcours',
    'A.Fiche signalétique : A-2. Station Départ',
    'A.Fiche signalétique : A-3. Station Destination',
    'A.Fiche signalétique : A-4. Station évaluée',
    'A.Fiche signalétique : A-7.Date',
    'Reclamation', 
    'Categorie']
    ,sep='\t')

#print dataset
print('My Dataset : \n', data1)

#get the bath to the new dataset
path = 'D:\mydata.txt'

#read the data and give the name to each column

data = pd.read_csv(path, header=None,names=['Reclamation', 'Categorie'],sep='\t')

#print data
print('My New Dataset : \n', data)
##################"""

#################""

#Checking for the label counts in the categorical parameters 
print(data['Reclamation'].shape)
print(data['Categorie'].value_counts())


#data description (count, mean,std,min, 25%, 50%, 75%, max)
print('My data describtion : \n')
print(data.describe())
print(data.agg(['count', 'nunique', 'min', 'max']))


#Draw the data#
col = 'Categorie'
fig, (ax1, ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(10,8))
explode = list((np.array(list(data[col].dropna().value_counts()))/sum(list(data[col].dropna().value_counts())))[::-1])[:]
labels = list(data[col].dropna().unique())[:]
sizes = data[col].value_counts()[:]
ax2.pie(sizes,  explode=explode, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.7)
ax2.add_artist(plt.Circle((0,0),0.6,fc='white'))
sns.countplot(y =col, data = data, ax=ax1)
ax1.set_title("Count of each categorie")
ax2.set_title("Percentage of each categorie")
plt.show()

##############
#Check if there is null data 
print ('Check if there is null data : \n',data.isnull().sum())

#Remove missing values (here to make review no null)
data.dropna(inplace=True)

#count the blanks in reviews
blanks = []

for i,lb,rv in data.itertuples():  
    if type(rv)==str:            
        if rv.isspace():         
            blanks.append(i)     
        
print(len(blanks), 'blanks: ', blanks)
print('\n ######################## \n')


#remove the blank
data.drop(blanks, inplace=True)
len(data)

#print data
print('My New Dataset : \n', data)

#########""
X = data['Reclamation']  
y = data['Categorie']



#separate data to training data and testig data
X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=0.30, random_state=42)

print('Training Data :\n', X_train.head(5))
print('Training Data :\n', X_train.shape)
print('Testing Data : \n', X_test.head(5))
print('Testing Data : \n', X_test.shape)
print('Training Y: \n', y_train.head(5))
print('Training Y: \n', y_train.shape)
print('Testing Y: \n', y_test.head(5))
print('Testing Y: \n', y_test.shape)


#Convert a collection of raw documents to a matrix of TF-IDF features.

vectorizer = TfidfVectorizer()
#X_train_tfidf = vectorizer.fit_transform(X_train)
#y_train_tfidf = vectorizer.fit_transform(y_train)

# remember to use the original X_train set
#print ('Convert the dataset using TfidfVectorizer: \n' ,X_train_tfidf[3])
#print ('Convert the dataset using TfidfVectorizer: \n' ,y_train_tfidf[3])

#Pipeline make a fast and triée step like dictionnary 

# svm.SVC(kernel='rbf', gamma = 0.8, C= 1.5,decision_function_shape='ovo')
model= svm.SVC(kernel='rbf', gamma = 0.58, C= 3)
ovo =  OneVsOneClassifier(model)
text_clf = Pipeline([('tfidf', vectorizer),
                    ('clf', ovo)])

clf =text_clf.fit(X_train, y_train)  
predictions = clf.predict(X_test)

#Compute confusion matrix to evaluate the accuracy of a classification.
CM = metrics.confusion_matrix(y_test,predictions)
labels = ['comportement', 'gestion', 'metier', 'respect' ,'sécurité','technique'   ]
sns.heatmap(CM,annot=True, fmt='d', center = True, xticklabels=labels, yticklabels=labels)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title("CONFUSION MATRIX \n", size=20)
plt.show()


#Draw the Test Data
fig, (ax1, ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(10,8))
explode = list((np.array(list(y_test.dropna().value_counts()))/sum(list(y_test.dropna().value_counts())))[::-1])[:]
labels = list(y_test.dropna().unique())[:]
sizes = y_test.value_counts()[:]
ax2.pie(sizes,  explode=explode, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.7)
ax2.add_artist(plt.Circle((0,0),0.6,fc='white'))
sns.countplot(y =y_test, data = data, ax=ax1)
ax1.set_title("Count of each categorie")
ax2.set_title("Percentage of each categorie")
plt.show()


#print('Matrice de Confusion: \n',CM)
report = metrics.classification_report(y_test,predictions)
accuracy = metrics.accuracy_score(y_test,predictions)
print('Report: \n', report) 
print('Accuracy =', accuracy, '%')

#eli.explain_weights(clf)
#eli.explain_prediction(clf , np.array(X_test)[1])
#eli.show_prediction(clf, X_test,
#                    feature_names=list(X),
#                    show_feature_values=True)


#print('\n ######################## \n')

XX = ['le kiosque est cassé']
print ('label: ', clf.predict(XX))


